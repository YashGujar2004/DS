{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a151d8d2-0d16-4c57-a1de-9e9c84db0be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_document = \"\"\"\n",
    "This is a sample document for demonstrating text preprocessing.\n",
    "It includes multiple sentences and some common words like is, a, for.\n",
    "We will perform tokenization, POS tagging, stop words removal, stemming, and lemmatization on this document.\n",
    "The goal is to understand the basic steps involved in preparing text data for further analysis.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c6af2a9-db4c-447b-af76-a283e1a53c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['this', 'is', 'a', 'sample', 'document', 'for', 'demonstrating', 'text', 'preprocessing', '.', 'it', 'includes', 'multiple', 'sentences', 'and', 'some', 'common', 'words', 'like', 'is', ',', 'a', ',', 'for', '.', 'we', 'will', 'perform', 'tokenization', ',', 'pos', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'and', 'lemmatization', 'on', 'this', 'document', '.', 'the', 'goal', 'is', 'to', 'understand', 'the', 'basic', 'steps', 'involved', 'in', 'preparing', 'text', 'data', 'for', 'further', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#nltk.download('punkt') \n",
    "#nltk.download('punkt_tab')\n",
    "tokens = word_tokenize(sample_document.lower()) \n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66b3825-b360-418b-80ac-47fe7f3728cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('this', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('document', 'NN'), ('for', 'IN'), ('demonstrating', 'VBG'), ('text', 'JJ'), ('preprocessing', 'NN'), ('.', '.'), ('it', 'PRP'), ('includes', 'VBZ'), ('multiple', 'JJ'), ('sentences', 'NNS'), ('and', 'CC'), ('some', 'DT'), ('common', 'JJ'), ('words', 'NNS'), ('like', 'IN'), ('is', 'VBZ'), (',', ','), ('a', 'DT'), (',', ','), ('for', 'IN'), ('.', '.'), ('we', 'PRP'), ('will', 'MD'), ('perform', 'VB'), ('tokenization', 'NN'), (',', ','), ('pos', 'NN'), ('tagging', 'NN'), (',', ','), ('stop', 'VB'), ('words', 'NNS'), ('removal', 'JJ'), (',', ','), ('stemming', 'VBG'), (',', ','), ('and', 'CC'), ('lemmatization', 'NN'), ('on', 'IN'), ('this', 'DT'), ('document', 'NN'), ('.', '.'), ('the', 'DT'), ('goal', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('understand', 'VB'), ('the', 'DT'), ('basic', 'JJ'), ('steps', 'NNS'), ('involved', 'VBN'), ('in', 'IN'), ('preparing', 'VBG'), ('text', 'NN'), ('data', 'NNS'), ('for', 'IN'), ('further', 'JJ'), ('analysis', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Download the Averaged Perceptron Tagger data\n",
    "#nltk.download('averaged_perceptron_tagger_eng')\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(\"POS Tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b254a9ca-cc24-41c1-a620-10851dcf0df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens (Stop Words Removed): ['sample', 'document', 'demonstrating', 'text', 'preprocessing', '.', 'includes', 'multiple', 'sentences', 'common', 'words', 'like', ',', ',', '.', 'perform', 'tokenization', ',', 'pos', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'lemmatization', 'document', '.', 'goal', 'understand', 'basic', 'steps', 'involved', 'preparing', 'text', 'data', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "print(\"Filtered Tokens (Stop Words Removed):\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e565df0-fb93-4d14-89a1-d99fb9622664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['sampl', 'document', 'demonstr', 'text', 'preprocess', '.', 'includ', 'multipl', 'sentenc', 'common', 'word', 'like', ',', ',', '.', 'perform', 'token', ',', 'po', 'tag', ',', 'stop', 'word', 'remov', ',', 'stem', ',', 'lemmat', 'document', '.', 'goal', 'understand', 'basic', 'step', 'involv', 'prepar', 'text', 'data', 'analysi', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "stemmed_tokens = [porter_stemmer.stem(token) for token in filtered_tokens]\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3390a933-b7fa-4a0a-8b1c-d3bb8cb80edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['sample', 'document', 'demonstrate', 'text', 'preprocessing', '.', 'include', 'multiple', 'sentence', 'common', 'word', 'like', ',', ',', '.', 'perform', 'tokenization', ',', 'po', 'tagging', ',', 'stop', 'word', 'removal', ',', 'stem', ',', 'lemmatization', 'document', '.', 'goal', 'understand', 'basic', 'step', 'involve', 'prepare', 'text', 'data', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# nltk.download('wordnet') \n",
    "# nltk.download('omw-1.4') \n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [wordnet_lemmatizer.lemmatize(token, pos='n') if tag.startswith('N') else \\\n",
    "                     wordnet_lemmatizer.lemmatize(token, pos='v') if tag.startswith('V') else \\\n",
    "                     wordnet_lemmatizer.lemmatize(token, pos='a') if tag.startswith('J') else \\\n",
    "                     wordnet_lemmatizer.lemmatize(token, pos='r') if tag.startswith('R') else \\\n",
    "                     wordnet_lemmatizer.lemmatize(token)\n",
    "                     for token, tag in pos_tags if token not in stop_words] # Apply lemmatization based on POS tag\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b066c6-90a6-4e05-ace4-a2a0c719f0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Corpus: [['sample', 'document', 'demonstrating', 'text', 'preprocessing'], ['includes', 'multiple', 'sentence', 'common', 'word', 'like'], ['perform', 'tokenization', 'po', 'tagging', 'stop', 'word', 'removal', 'stemming', 'lemmatization', 'document'], ['goal', 'understand', 'basic', 'step', 'involved', 'preparing', 'text', 'data', 'analysis']]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "\"This is a sample document for demonstrating text preprocessing.\",\n",
    "\"It includes multiple sentences and some common words like is, a, for.\",\n",
    "\"We will perform tokenization, POS tagging, stop words removal, stemming, and lemmatization on this document.\",\n",
    "\"The goal is to understand the basic steps involved in preparing text data for further analysis.\"\n",
    "]\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize text\n",
    "    tokens = [t for t in tokens if t not in stop_words and t not in string.punctuation]  \n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]  \n",
    "    return tokens\n",
    "\n",
    "# Process the corpus\n",
    "processed_corpus = [preprocess(doc) for doc in corpus]\n",
    "print(\"Processed Corpus:\", processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40354696-45a1-4a36-8ce4-ccfec60c38e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency (TF) for each document: [{'sample': 1, 'document': 1, 'demonstrating': 1, 'text': 1, 'preprocessing': 1}, {'includes': 1, 'multiple': 1, 'sentence': 1, 'common': 1, 'word': 1, 'like': 1}, {'perform': 1, 'tokenization': 1, 'po': 1, 'tagging': 1, 'stop': 1, 'word': 1, 'removal': 1, 'stemming': 1, 'lemmatization': 1, 'document': 1}, {'goal': 1, 'understand': 1, 'basic': 1, 'step': 1, 'involved': 1, 'preparing': 1, 'text': 1, 'data': 1, 'analysis': 1}]\n"
     ]
    }
   ],
   "source": [
    "def calculate_tf(document):\n",
    "    tf_dict = {}\n",
    "    for term in document:\n",
    "        tf_dict[term] = tf_dict.get(term, 0) + 1\n",
    "    return tf_dict\n",
    "\n",
    "tf_representations = [calculate_tf(doc) for doc in processed_corpus]\n",
    "print(\"Term Frequency (TF) for each document:\", tf_representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074d2c5-0734-4306-8ba7-5c5057a7b0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse Document Frequency (IDF) for each term: {'basic': 0.6931471805599453, 'includes': 0.6931471805599453, 'analysis': 0.6931471805599453, 'goal': 0.6931471805599453, 'lemmatization': 0.6931471805599453, 'understand': 0.6931471805599453, 'word': 0.28768207245178085, 'multiple': 0.6931471805599453, 'data': 0.6931471805599453, 'preprocessing': 0.6931471805599453, 'common': 0.6931471805599453, 'stop': 0.6931471805599453, 'perform': 0.6931471805599453, 'step': 0.6931471805599453, 'po': 0.6931471805599453, 'removal': 0.6931471805599453, 'preparing': 0.6931471805599453, 'involved': 0.6931471805599453, 'stemming': 0.6931471805599453, 'sentence': 0.6931471805599453, 'text': 0.28768207245178085, 'sample': 0.6931471805599453, 'like': 0.6931471805599453, 'demonstrating': 0.6931471805599453, 'tokenization': 0.6931471805599453, 'document': 0.28768207245178085, 'tagging': 0.6931471805599453}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def calculate_idf(corpus):\n",
    "    N = len(corpus)\n",
    "    idf_dict = {}\n",
    "    all_terms = set([term for doc in corpus for term in doc])\n",
    "    for term in all_terms:\n",
    "        df = sum(1 for doc in corpus if term in doc)\n",
    "        idf_dict[term] = math.log(N / (df + 1)) # Adding 1 to avoid division by zero\n",
    "    return idf_dict\n",
    "\n",
    "idf_values = calculate_idf(processed_corpus)\n",
    "print(\"Inverse Document Frequency (IDF) for each term:\", idf_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce084778-c352-4df6-bf55-2184c3945a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF representation for each document: [{'sample': 0.6931471805599453, 'document': 0.28768207245178085, 'demonstrating': 0.6931471805599453, 'text': 0.28768207245178085, 'preprocessing': 0.6931471805599453}, {'includes': 0.6931471805599453, 'multiple': 0.6931471805599453, 'sentence': 0.6931471805599453, 'common': 0.6931471805599453, 'word': 0.28768207245178085, 'like': 0.6931471805599453}, {'perform': 0.6931471805599453, 'tokenization': 0.6931471805599453, 'po': 0.6931471805599453, 'tagging': 0.6931471805599453, 'stop': 0.6931471805599453, 'word': 0.28768207245178085, 'removal': 0.6931471805599453, 'stemming': 0.6931471805599453, 'lemmatization': 0.6931471805599453, 'document': 0.28768207245178085}, {'goal': 0.6931471805599453, 'understand': 0.6931471805599453, 'basic': 0.6931471805599453, 'step': 0.6931471805599453, 'involved': 0.6931471805599453, 'preparing': 0.6931471805599453, 'text': 0.28768207245178085, 'data': 0.6931471805599453, 'analysis': 0.6931471805599453}]\n"
     ]
    }
   ],
   "source": [
    "def calculate_tfidf(tf_dict, idf_values):\n",
    "    tfidf_dict = {}\n",
    "    for term, tf in tf_dict.items():\n",
    "        tfidf_dict[term] = tf * idf_values.get(term, 0)\n",
    "    return tfidf_dict\n",
    "\n",
    "tfidf_representations = [calculate_tfidf(tf, idf_values) for tf in tf_representations]\n",
    "print(\"TF-IDF representation for each document:\", tfidf_representations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
