{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c47fbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yashg_t6wet39\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yashg_t6wet39\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yashg_t6wet39\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\yashg_t6wet39\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Download required NLTK resources (run only once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "855aa60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document:\n",
      " This is a sample document for demonstrating text preprocessing. It includes multiple sentences and some common words like is, a, for. We will perform tokenization, POS tagging, stop words removal, stemming, and lemmatization on this document. The goal is to understand the basic steps involved in preparing text data for further analysis.\n",
      "\n",
      "Tokenized Words:\n",
      " ['This', 'is', 'a', 'sample', 'document', 'for', 'demonstrating', 'text', 'preprocessing', '.', 'It', 'includes', 'multiple', 'sentences', 'and', 'some', 'common', 'words', 'like', 'is', ',', 'a', ',', 'for', '.', 'We', 'will', 'perform', 'tokenization', ',', 'POS', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'and', 'lemmatization', 'on', 'this', 'document', '.', 'The', 'goal', 'is', 'to', 'understand', 'the', 'basic', 'steps', 'involved', 'in', 'preparing', 'text', 'data', 'for', 'further', 'analysis', '.']\n",
      "\n",
      "POS Tags:\n",
      " [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('document', 'NN'), ('for', 'IN'), ('demonstrating', 'VBG'), ('text', 'JJ'), ('preprocessing', 'NN'), ('.', '.'), ('It', 'PRP'), ('includes', 'VBZ'), ('multiple', 'JJ'), ('sentences', 'NNS'), ('and', 'CC'), ('some', 'DT'), ('common', 'JJ'), ('words', 'NNS'), ('like', 'IN'), ('is', 'VBZ'), (',', ','), ('a', 'DT'), (',', ','), ('for', 'IN'), ('.', '.'), ('We', 'PRP'), ('will', 'MD'), ('perform', 'VB'), ('tokenization', 'NN'), (',', ','), ('POS', 'NNP'), ('tagging', 'NN'), (',', ','), ('stop', 'VB'), ('words', 'NNS'), ('removal', 'JJ'), (',', ','), ('stemming', 'VBG'), (',', ','), ('and', 'CC'), ('lemmatization', 'NN'), ('on', 'IN'), ('this', 'DT'), ('document', 'NN'), ('.', '.'), ('The', 'DT'), ('goal', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('understand', 'VB'), ('the', 'DT'), ('basic', 'JJ'), ('steps', 'NNS'), ('involved', 'VBN'), ('in', 'IN'), ('preparing', 'VBG'), ('text', 'NN'), ('data', 'NNS'), ('for', 'IN'), ('further', 'JJ'), ('analysis', 'NN'), ('.', '.')]\n",
      "\n",
      "Tokens after Stopword Removal:\n",
      " ['sample', 'document', 'demonstrating', 'text', 'preprocessing', 'includes', 'multiple', 'sentences', 'common', 'words', 'like', 'perform', 'tokenization', 'POS', 'tagging', 'stop', 'words', 'removal', 'stemming', 'lemmatization', 'document', 'goal', 'understand', 'basic', 'steps', 'involved', 'preparing', 'text', 'data', 'analysis']\n",
      "\n",
      "Stemmed Words:\n",
      " ['sampl', 'document', 'demonstr', 'text', 'preprocess', 'includ', 'multipl', 'sentenc', 'common', 'word', 'like', 'perform', 'token', 'po', 'tag', 'stop', 'word', 'remov', 'stem', 'lemmat', 'document', 'goal', 'understand', 'basic', 'step', 'involv', 'prepar', 'text', 'data', 'analysi']\n",
      "\n",
      "Lemmatized Words:\n",
      " ['sample', 'document', 'demonstrating', 'text', 'preprocessing', 'includes', 'multiple', 'sentence', 'common', 'word', 'like', 'perform', 'tokenization', 'POS', 'tagging', 'stop', 'word', 'removal', 'stemming', 'lemmatization', 'document', 'goal', 'understand', 'basic', 'step', 'involved', 'preparing', 'text', 'data', 'analysis']\n"
     ]
    }
   ],
   "source": [
    " # Step 1: Sample document\n",
    "doc = \"This is a sample document for demonstrating text preprocessing. It includes multiple sentences and some common words like is, a, for. We will perform tokenization, POS tagging, stop words removal, stemming, and lemmatization on this document. The goal is to understand the basic steps involved in preparing text data for further analysis.\"\n",
    "print(\"Original Document:\\n\", doc)\n",
    " \n",
    " # Step 2: Tokenization\n",
    "tokens = word_tokenize(doc)\n",
    "print(\"\\nTokenized Words:\\n\", tokens)\n",
    "\n",
    " # Step 3: POS Tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(\"\\nPOS Tags:\\n\", pos_tags)\n",
    "\n",
    " # Step 4: Stopwords Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "print(\"\\nTokens after Stopword Removal:\\n\", filtered_tokens)\n",
    "\n",
    "#Step 5: Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"\\nStemmed Words:\\n\", stemmed)\n",
    "\n",
    " # Step 6: Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens] \n",
    "print(\"\\nLemmatized Words:\\n\", lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960987aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Term Frequency (TF):\n",
      "    analysis  and  basic  common  data  demonstrating  document  for  further  \\\n",
      "0         0    0      0       0     0              1         1    1        0   \n",
      "1         0    1      0       1     0              0         0    1        0   \n",
      "2         0    1      0       0     0              0         1    0        0   \n",
      "3         1    0      1       0     1              0         0    1        1   \n",
      "\n",
      "   goal  ...  tagging  text  the  this  to  tokenization  understand  we  \\\n",
      "0     0  ...        0     1    0     1   0             0           0   0   \n",
      "1     0  ...        0     0    0     0   0             0           0   0   \n",
      "2     0  ...        1     0    0     1   0             1           0   1   \n",
      "3     1  ...        0     1    2     0   1             0           1   0   \n",
      "\n",
      "   will  words  \n",
      "0     0      0  \n",
      "1     0      1  \n",
      "2     1      1  \n",
      "3     0      0  \n",
      "\n",
      "[4 rows x 40 columns]\n",
      "\n",
      "TF-IDF:\n",
      "    analysis       and     basic    common      data  demonstrating  document  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000       0.419606  0.330822   \n",
      "1  0.000000  0.261961  0.000000  0.332264  0.000000       0.000000  0.000000   \n",
      "2  0.000000  0.214687  0.000000  0.000000  0.000000       0.000000  0.214687   \n",
      "3  0.246659  0.000000  0.246659  0.000000  0.246659       0.000000  0.000000   \n",
      "\n",
      "        for   further      goal  ...   tagging      text       the      this  \\\n",
      "0  0.267829  0.000000  0.000000  ...  0.000000  0.330822  0.000000  0.330822   \n",
      "1  0.212080  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.000000  0.000000  ...  0.272303  0.000000  0.000000  0.214687   \n",
      "3  0.157439  0.246659  0.246659  ...  0.000000  0.194469  0.493317  0.000000   \n",
      "\n",
      "         to  tokenization  understand        we      will     words  \n",
      "0  0.000000      0.000000    0.000000  0.000000  0.000000  0.000000  \n",
      "1  0.000000      0.000000    0.000000  0.000000  0.000000  0.261961  \n",
      "2  0.000000      0.272303    0.000000  0.272303  0.272303  0.214687  \n",
      "3  0.246659      0.000000    0.246659  0.000000  0.000000  0.000000  \n",
      "\n",
      "[4 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    " # Step 7: TF and TF-IDF\n",
    " # Sample corpus\n",
    "corpus = [\n",
    "\"This is a sample document for demonstrating text preprocessing.\",\n",
    "\"It includes multiple sentences and some common words like is, a, for.\",\n",
    "\"We will perform tokenization, POS tagging, stop words removal, stemming, and lemmatization on this document.\",\n",
    "\"The goal is to understand the basic steps involved in preparing text data for further analysis.\"\n",
    "]\n",
    "\n",
    " # Term Frequency (TF)\n",
    "cv = CountVectorizer()\n",
    "X_tf = cv.fit_transform(corpus)\n",
    "tf_df = pd.DataFrame(X_tf.toarray(), columns=cv.get_feature_names_out())\n",
    "print(\"\\nTerm Frequency (TF):\\n\", tf_df)\n",
    "\n",
    "# Inverse Document Frequency (TF-IDF)\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(corpus)\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
    "print(\"\\nTF-IDF:\\n\", tfidf_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
